# ğŸ§  Ethereum Address-Level Fraud Detection (GNN â€¢ TabTransformer â€¢ LightGBM)

This repository implements a **secure, leakage-free, and reproducible AI pipeline** for Ethereum address-level fraud detection.  
It combines **graph-based**, **transformer-based**, and **gradient boosting** models with verified data integrity, based on the paper *â€œGreenLand: A Secure Land Registration Scheme for Blockchain and AI-Enabled Agriculture Industry 5.0â€*.

## ğŸ“˜ Overview
The goal is to detect fraudulent blockchain addresses using a hybrid of **graph learning (GNN)**, **tabular deep learning (TabTransformer)**, and **gradient boosting (LightGBM)** on Ethereum transaction-level features.  
Each model is evaluated on **exactly the same canonical test set**, ensuring fair comparison and zero data leakage.

## ğŸš€ Key Highlights
âœ… Group-aware splitting (no address overlap).  
âœ… Train-only preprocessing (no leakage).  
âœ… Rare-value sanitization for categorical features.  
âœ… Unified output naming for automatic evaluation.  
âœ… Verified reproducibility and fairness.

## ğŸ§© Pipeline Overview
### 1ï¸âƒ£ Data Preprocessing
- Deduplicated addresses (numeric=mean, categorical=mode).  
- Used GroupShuffleSplit: 80/20, then 15% of train â†’ validation (â‰ˆ68/12/20).  
- Train-only fitting for LabelEncoders + StandardScaler.  
- Saved canonical splits:
  - X_train.npy / X_val.npy / X_test.npy
  - y_train.npy / y_val.npy / y_test.npy
  - addresses_train.npy / addresses_val.npy / addresses_test.npy
  - preproc_train_encoders.pkl

### 2ï¸âƒ£ Model Training
- **LightGBM:** GBDT with early stopping via callbacks.  
- **GraphSAGE (GNN):** Aligned node indices via address mapping.  
- **TabTransformer:** Reused canonical splits; no new random split.  
- **Baselines:** Logistic Regression & SVM on same data.

### 3ï¸âƒ£ Model Comparison
- Robust auto-discovery cell to load correct *_test_probs.npy per model.  
- Computes ROC-AUC, PR-AUC, Precision, Recall, F1, Accuracy.  
- Generates metrics table, ROC, PR, and confusion matrix plots.

## ğŸ“Š Final Results (Canonical Test Set, n=1964)
| Model | ROC-AUC | PR-AUC | Precision | Recall | F1 | Accuracy |
|--------|----------|--------|------------|---------|----|-----------|
| LightGBM | **0.9992** | **0.9981** | 1.000 | 0.972 | 0.986 | 0.994 |
| SVM | 0.9952 | 0.9868 | 0.988 | 0.919 | 0.952 | 0.980 |
| TabTransformer | 0.9949 | 0.9869 | 0.997 | 0.907 | 0.950 | 0.979 |
| GNN (GraphSAGE) | 0.9896 | 0.9816 | 0.926 | 0.937 | 0.932 | 0.970 |
| LogReg | 0.9180 | 0.8226 | 0.778 | 0.870 | 0.821 | 0.917 |

## ğŸ§  Findings
- LightGBM outperforms deep models on structured Ethereum data.  
- Deep models improve after alignment with canonical splits.  
- No leakage detected (validated via ablation & permutation tests).

## ğŸ“ Repository Structure
```
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ X_train.npy, X_val.npy, X_test.npy
â”‚   â”œâ”€â”€ y_train.npy, y_val.npy, y_test.npy
â”‚   â”œâ”€â”€ addresses_train.npy, addresses_val.npy, addresses_test.npy
â”‚   â”œâ”€â”€ preproc_train_encoders.pkl
â”‚   â”œâ”€â”€ *_test_probs.npy / *_test_preds.npy / *_test_true.npy
â”‚   â”œâ”€â”€ models_comparison_metrics.csv
â”‚   â””â”€â”€ lightgbm_booster.txt
â”œâ”€â”€ preprocessing_group_aware.py
â”œâ”€â”€ build_knn_graph.py
â”œâ”€â”€ train_lightgbm.py
â”œâ”€â”€ train_gnn.py
â”œâ”€â”€ train_tabtransformer.py
â”œâ”€â”€ compare_models.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§¾ Citation
> Vineeth C S, *Ethereum Address-Level Fraud Detection via Graph and Transformer Models*, 2025.

## ğŸ§ª Environment
- Python â‰¥ 3.10  
- scikit-learn â‰¥ 1.3  
- LightGBM â‰¥ 4.3  
- PyTorch â‰¥ 2.1  
- torch-geometric â‰¥ 2.4  
- pandas, numpy, matplotlib, seaborn

## ğŸ Next Steps
- Temporal hold-out evaluation.  
- Cross-validation (5-fold AUC).  
- SHAP interpretability analysis.  
- Publish sanitized dataset for reproducibility.

---
**Maintainer:** C S Vineeth  
AI / Blockchain Researcher  
